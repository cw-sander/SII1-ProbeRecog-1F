---
title: "Analyses for SII Experiment 2"
output: 
  html_notebook:
  code_folding: hide
---

```{r setup, include = FALSE}
# R version 4.2.1
# load packages & set options
library(dplyr) # dplyr_1.0.10
library(ggplot2) # ggplot2_3.3.6
library(ggpubr) # ggpubr_0.4.0
library(ggrepel, include.only = "geom_label_repel") # ggrepel_0.9.1
library(here) # here_1.0.1
library(lmerTest) # lmerTest_3.1-3
library(magrittr, include.only = "%T>%") # magrittr_2.0.3
library(rstatix) # rstatix_0.7.0
library(showtext) # showtext_0.9-5
library(tidyverse) # tidyverse_1.3.2

# Read data
d_long <- readRDS(here::here("Processed data/d-long.rds"))

# Demographics
dems <- d_long %>%
  select(subject_id, age, gender, starts_with("pol_")) %>%
  unique()

# Add custom font for plots
font_add("Nunito",
  regular = "/Users/carsten/Library/Fonts/NunitoSans-Regular.ttf",
  italic = "/Users/carsten/Library/Fonts/NunitoSans-Italic.ttf",
  bold = "/Users/carsten/Library/Fonts/NunitoSans-Bold.ttf",
  bolditalic = "/Users/carsten/Library/Fonts/NunitoSans-BoldItalic.ttf")
showtext_auto()

# Custom functions
formp <- function(p, text = FALSE) {
  ## ---------------------------
  ## Format p values
  ##
  ## This function takes in a number between
  ## zero and one or a formatted p-value and outputs
  ## a formatted p-value. If p-value is already formatted
  ## then applying the function changes the format from
  ## "p = .034" to ".034" and vice versa.
  ##
  ## @p p-value to be formatted
  ## @text adds "p = " or "p < " to output
  ##
  ## @out string with formatted p-value
  ## ---------------------------

  # If already formatted but no "p" then add "p"
  if (grepl("^<.\\d{3}$", p)) {
  out <- gsub("<", "p < ", p)
  } else if (grepl("^.\\d{3}$", p)) {
  out <- gsub("^", "p = ", p)
  # If already formatted and "p" then remove "p"
  } else if (grepl("^p < .\\d{3}$", p)) {
  out <- gsub("p < ", "<", p)
  } else if (grepl("^p = .\\d{3}$", p)) {
  out <- gsub("p = ", "", p)
  # If not yet formatted and smaller than .001
  } else if (is.numeric(p) && p < 0.001) {
  if (text) {
    out <- "p < .001"
  } else {
    out <- "<.001"
  }
  # If not yet formatted and bigger than .001
  } else if (p >= 0.001) {
  p <- format(round(p, 3), nsmall = 3, scientific = FALSE)
  p <- sub("0.", ".", p)
  if (text) {
    out <- paste("p =", p)
  } else {
    out <- p
  }
  }
  return(out)
}
forma <- function(number, dec = NULL, lead_zero = TRUE) { # nolint
  ## ---------------------------
  ## Format values in apa style
  ##
  ## This function takes in a number and outputs
  ## a formatted number. If no decimal is provided, then
  ## it uses a heuristic to round the number. If lead_zero
  ## is set to FALSE, then the lead zero of the number is
  ## removed (useful for p-values or eta squared).
  ##
  ## @number input number
  ## @dec optional number of decimals
  ## @lead_zero keep leading zero
  ##
  ## @out formatted number
  ## ---------------------------

  # If dec is logical, interpret as lead_zero
  if (is.logical(dec)) {
  lead_zero <- dec
  dec <- NULL
  }
  # If no decimal is specified, use heuristic
  if (!is.null(dec)) {
  } else if (abs(number) >= 100) {
    dec <- 0
  } else if (abs(number) >= 10 && number < 100) {
    dec <- 1
  } else if (abs(number) >= 0.1 && number < 10) {
    dec <- 2
  } else if (abs(number) >= 0.001 && number < 0.1) {
    dec <- 3
  } else if (abs(number) < 0.001 && number != 0) {
    dec <- stringr::str_locate(format(
      abs(number), scientific = FALSE), "[1-9]{1}")[1] - 2
  } else if (number == 0) {
    dec <- 0
  }
  # Round number to decimal
  out <- format(round(number, dec), nsmall = dec, scientific = FALSE)
  # Remove leading zero if required
  if (out < 1 && lead_zero == FALSE) {
  out <- sub("0.", ".", out)
  }
  return(out)
}
cor_table <- function(data, method = c("pearson", "spearman")) {
  # Compute correlation matrix
  pvalues <- data %>%
  cor_pmat(method = method[1]) %>%
  rowwise() %>%
  mutate(across(!1, formp))
  coefs <- data %>%
  cor_mat(method = method[1]) %>%
  rowwise() %>%
  mutate(across(!1, \(x) forma(x, 2)))
  for (row in seq(2, nrow(coefs))) {
  for (col in seq(2, ncol(coefs) - 1)) {
    c <- coefs[row, col]
    p <- pvalues[row, col]
    coefs[row, col] <- paste0(c, " (", p, ")")
  }
  }
  coefs <- coefs %>%
  pull_lower_triangle() %>%
  slice(-1) %>%
  select(-last_col()) %>%
  rename(variable = 1)
  return(coefs)
}
mlm_compare <- function(new, old) {
  ## ---------------------------
  ## Compare two multilevel models
  ##
  ## Outputs a list with a test comparing the models ($test),
  ## a formatted p-value ($p), the variance explained by the
  ## random and fixed effects ($var_exp) and the percentage
  ## increase in variance explained in the new compared to
  ## the old model.
  ##
  ## @new new model
  ## @old old model to compare the new
  ##    model against
  ##
  ## @out list with statistics
  ## ---------------------------

  out <- list()

  # Model comparison
  out$test <- anova(new, old)
  out$p <- formp(out$test[2, "Pr(>Chisq)"], TRUE)

  # Stats new model
  var_new <- as.data.frame(lme4::VarCorr(new))
  var_exp <- data.frame(row.names = c("old", "new"))
  # Loop across random effects
  for (i in seq_along(var_new[, 1])) {
  var_exp_i <- var_new$vcov[i] / sum(var_new$vcov) * 100
  var_exp[2, var_new$grp[i]] <- var_exp_i
  }

  # Stats old model
  if (class(old) != "lm") {
  var_old <- as.data.frame(lme4::VarCorr(old))
  # Loop across random effects
  for (i in seq_along(var_old[, 1])) {
    var_exp_i <- var_old$vcov[i] / sum(var_old$vcov) * 100
    var_exp[1, var_old$grp[i]] <- var_exp_i
  }
  var_exp["diff", ] <- var_exp[1, ] - var_exp[2, ]
  var_exp_delta <- ((var_exp[3, ] / var_exp[1, ]) * 100)
  var_exp_delta <- sapply(var_exp_delta, forma)
  var_exp[] <- apply(var_exp, c(1, 2), forma)
  out$var_exp_delta <- var_exp_delta
  } else if (class(old) == "lm") {
  var_exp[2, ] <- sapply(var_exp[2, ], forma)
  }
  out$var_exp <- var_exp
  return(out)
}
theme_cs_talk <- function(font = "Nunito", lab_size = 16, label_size = 14,
  dark = "#7A8450", light = "#F7FF8C", solid_facet = TRUE) {
  if (solid_facet) {
  facet_fill <- dark
  facet_text <- light
  } else if (!solid_facet) {
  facet_fill <- "transparent"
  facet_text <- dark
  }
  theme_bw(base_size = 16) %+replace%
  theme(
  # Rectangle elements
  plot.background = element_rect(fill = "transparent",
    color = NA_character_),
  panel.background = element_rect(fill = "transparent"),
  legend.background = element_rect(fill = "transparent", color = NA),
  strip.background = element_rect(color = facet_fill,
    fill = facet_fill, linewidth = 1),
  panel.border = element_rect(color = dark, fill = NA, linewidth = 1),
  legend.key = element_rect(fill = "transparent", color = NA_character_),
  # Text elements
  plot.title = element_text(family = font, size = lab_size,
    face = "bold", hjust = 0, vjust = 2, color = dark),
  plot.subtitle = element_text(family = font,
    size = lab_size - 2, color = dark),
  plot.caption = element_text(family = font, size = lab_size,
    hjust = 1, color = dark),
  axis.title = element_text(family = font, size = lab_size,
    color = dark),
  axis.text = element_text(family = font, size = label_size,
    color = dark),
  axis.text.x = element_text(margin = margin(5, b = 10),
    color = dark),
  legend.title = element_text(family = font, size = lab_size,
    color = dark, hjust = 0),
  legend.text = element_text(family = font, size = label_size,
    color = dark),
  strip.text = element_text(family = font, size = label_size,
    color = facet_text, margin = margin(4, 4, 4, 4)),
  # Line elements
  axis.ticks = element_line(color = dark, linewidth = 0.5),
  panel.grid.major = element_blank(),
  panel.grid.minor = element_blank()
  )
}
```

### Sample Characteristics
We collected data from a total of N = 170 participants. Following our pre-registered exclusion criteria, we excluded 5 participants whose average correct response times were slower than two standard deviations over the sample mean, 1 who self-reported not having followed the instructions conscientiously, and 1 who rated their own data to be unfit for analysis. This resulted in a sample of N = `r nrow(dems)` participants (`r nrow(filter(dems, gender == "female"))` female, `r nrow(filter(dems, gender == "male"))` male, `r nrow(filter(dems, gender == "other"))` other, `r nrow(filter(dems, gender %in% c("not specified","")))` not specified; average age M = `r round(mean(dems$age, na.rm = TRUE), 1)` years, SD = `r round(sd(dems$age, na.rm = TRUE), 1)`, ranging from `r min(dems$age, na.rm = TRUE)` to `r max(dems$age, na.rm = TRUE)`). Participants were recruited via the online platform Prolific (www.prolific.co) and received monetary compensation of 3.13 GBP for completing the 25-minute study. An additional 27 people started the experiment on prolific but either returned their submission, timed-out, or only partially completed the experiment due to technical issues.

On average, participants reported to be rather left leaning (M = `r round(mean(dems$pol_orientation), 1)`, SD = `r round(sd(dems$pol_orientation), 1)` on a scale from 1 = left to 10 = right), rather interested in politics (M = `r round(mean(dems$pol_interest), 1)`, SD = `r round(sd(dems$pol_interest), 1)`, on a scale ranging from 1 = not at all to 10 = very strongly), and moderately satisfied with the German political system (M = `r round(mean(dems$pol_satisfaction), 1)`, SD = `r round(sd(dems$pol_satisfaction), 1)`, on a scale ranging from 1 = satisfied to 4 = dissatisfied).

### Preregistered analysis {.tabset}
```{r rt analysis, warning = FALSE, message = FALSE}
# Prepare reaction time data
rt <- d_long %>%
  mutate(across(starts_with("rt_"), ~ifelse(is_correct == 0, NA, .))) %>%
  select(subject_id, probe_type, starts_with("rt_")) %>%
  mutate(probe_type = recode(probe_type,
    "im" = "implied", "io" = "implied other")) %>%
  group_by(probe_type, subject_id) %>%
  summarize(.groups = "drop_last", across(everything(), \(x) mean(x, na.rm = TRUE))) %>%
  # Find extreme outliers
  mutate(is_extreme = is_extreme(rt_M2SD_log)) %>%
  ungroup() %T>%
  # Count participants that have extreme outliers
  assign(x = "rt_ex_n", envir = .GlobalEnv, value = n_distinct(
    select(filter(., is_extreme == TRUE), subject_id)))

# Create data frames where these rows are excluded
rt_ex <- filter(rt, is_extreme == FALSE)

# Descriptives
rt_desc <- rt_ex %>%
  group_by(probe_type) %>%
  mutate(rt_M2SD_log_exp = exp(rt_M2SD_log)) %>%
  get_summary_stats(rt_M2SD_log_exp, type = "mean_sd") %>%
  mutate(
    ci95_low = mean - 1.96 * sd / sqrt(n),
    ci95_upp = mean + 1.96 * sd / sqrt(n))

# One-sided paired t-test
rt_t <- rt_ex %>%
  t_test(
    rt_M2SD_log ~ probe_type,
    paired = TRUE,
    alternative = "greater",
    ref.group = "implied")

# Cohens dz
rt_dz <- rt_ex %>%
  cohens_d(rt_M2SD_log ~ probe_type,
    paired = TRUE, ref.group = "implied") %>%
  pull(effsize)

# Print t-test
rt_t_report <- paste0("t(", rt_t$df, ") = ", forma(rt_t$statistic),
  ", ", formp(rt_t$p, TRUE), ", d~z~ = ", forma(rt_dz))
rt_t_plot <- paste0("t(", rt_t$df, ") = ", forma(rt_t$statistic),
  ",\n", formp(rt_t$p, TRUE), ",\nd~z~ = ", forma(rt_dz))
```

To prepare the response latency data for our main anlysis we applied an individual cut-off of the individual mean plus two standard deviations for slow responses and the log-transformation and then computed the average for each experimental condition. For reports of descriptive data we backtransformed the logarithmized values by calculating their exponentials.

#### Outlier detection
```{r rt outlier, message = FALSE}
rt_outlier_plot <- rt %>%
  ggplot(., aes(probe_type, rt_M2SD_log)) +
  geom_boxplot(color = "#7A8450") +
  labs(x = "Probe type", y = "Mean response latency in log(s)") +
  theme_cs_talk()
rt_outlier_plot
```

We looked for extreme outliers in each cell of the design. We defined them as values above Q3 + 3 * IQR or below Q1 - 3 * IQR. We found and excluded `r rt_ex_n` participants whose values were extreme outliers.

#### QQ-Plot
```{r rt qq, message = FALSE}
rt_qq_plot <- rt_ex %>%
  pivot_wider(names_from = probe_type, values_from = starts_with("rt_")) %>%
  mutate(diff = rt_M2SD_log_implied - `rt_M2SD_log_implied other`) %>%
  ggplot(., aes(sample = diff)) +
  labs(x = "Theoretical quantiles", y = "Data quantiles") +
  stat_qq(color = "#7A8450") +
  stat_qq_line(color = "#7A8450") +
  theme_cs_talk()
rt_qq_plot
```

Upon visual inspection the differences between the implied and implied-other conditions did not seem severely non-normal. We therefore applied no further transformations before conducting a paired t-test.

#### T-test
On average, participants responded significantly slower in the implied condition (M = `r forma(rt_desc$mean[1])`, SD = `r forma(rt_desc$sd[1])`) than in the implied other condition (M = `r forma(rt_desc$mean[2])`, SD = `r forma(rt_desc$sd[2])`), `r rt_t_report`. This indicates that participants spontaneously activated the implied labels while reading the statements.

#### Descriptives
```{r rt descriptives}
knitr::kable(rt_desc, format = "markdown")
```

#### Barplot
```{r rt barplot, message = FALSE}
rt_barplot <- rt_desc %>%
  ggplot(., aes(probe_type, mean)) +
  labs(x = "Probe type", y = "Mean response latency in ms") +
  geom_bar(stat = "identity", color = "#7A8450", fill = "white") +
  coord_cartesian(ylim = c(650, 870)) +
  geom_linerange(aes(ymin = ci95_low, ymax = ci95_upp), color = "#7A8450") +
  geom_label(aes(x = 2.08, y = 0.85, label = rt_t_plot),
    fill = "white", color = "#7A8450", show.legend = FALSE,
    label.padding = unit(0.5, "lines"), label.size = 0.5) +
  theme_cs_talk()
rt_barplot
```

#### Multiverse
```{r rt mv, message = FALSE}
rt_mv <- d_long %>%
  mutate(across(starts_with("rt_"), ~ifelse(is_correct == 0, NA, .))) %>%
  select(probe_type, subject_id, starts_with("rt_")) %>%
  mutate(probe_type =
    recode(probe_type, "im" = "implied", "io" = "implied other")) %>%
  group_by(probe_type, subject_id) %>%
  summarize(.groups = "drop",
    across(everything(), \(x) mean(x, na.rm = TRUE))) %>%
  pivot_longer(
    cols = starts_with("rt"),
    names_to = c(NA, "cutoff", "trans"),
    values_to = "rt",
    names_sep = "_")

# Loop over all combinations of cutoffs and transformations
mv <- data.frame(
  cutoff = rep(c("none", "M2SD", "f25", "f20", "f15"), 3),
  transformation = c(rep("none", 5), rep("log", 5), rep("inv", 5)))
for (c in seq_along(mv[, 1])) {
  dat <- rt_mv %>%
    # Filter data using c-th cutoff and transformation
    filter(
      cutoff == mv$cutoff[c] &
      trans == mv$transformation[c]) %>%
    group_by(probe_type) %>%
    mutate(is_extreme = is_extreme(rt)) %>%
    ungroup() %T>%
    assign(x = "ex_n", envir = .GlobalEnv, value = n_distinct(
      select(filter(., is_extreme == TRUE), subject_id))) %>%
    filter(is_extreme == FALSE) %>%
    group_by(subject_id) %>%
    filter(n() == 2) %>%
    ungroup()
  test <- dat %>%
    t_test(rt ~ probe_type, paired = TRUE,
      alternative = "greater", ref.group = "implied") %>%
    rowwise() %>%
    mutate(statistic = forma(statistic, 2), p = formp(p))
  dz <- dat %>%
    cohens_d(rt ~ probe_type, paired = TRUE, ref.group = "implied") %>%
    pull(effsize) %>%
    forma()
  # Save test statistics
  mv$ex_n[c] <- ex_n
  mv$t[c] <- test$statistic
  mv$df[c] <- test$df
  mv$p[c] <- test$p
  mv$d_z[c] <- dz
}
# Print multiverse table
knitr::kable(mv, format = "markdown")
```

Because there are no conventions regarding outlier correction and transformations, we follow the example of Krieglmeyer & Deutsch (2010) and employ a multiverse analysis using different cut-off criteria (none, individual M + 2 SD, 2500 ms, 2000 ms, and 1500 ms) and transformations (none, log, inverse) and report each combination's effects on the results.

The effect of probe type was highly significant across all combinations of cutoff criteria and transformations. The effect size ranged from d~z~ = `r mv$d_z[1]` for untransformed response latencies with no cutoff to d~z~ = `r mv$d_z[10]` for log transformed response latencies with a 1500 ms cutoff.

### Exploratory error rate analysis {.tabset}
```{r er analysis, message = FALSE}
# Prepare error rate data
er <- d_long %>%
  select(subject_id, probe_type, is_correct) %>%
    mutate(probe_type = recode(probe_type,
      "im" = "implied", "io" = "implied other")) %>%
  group_by(probe_type, subject_id) %>%
  summarize(
    .groups = "drop_last",
    errors = sum(is_correct == 0),
    n_trials = n()) %>%
  mutate(er = errors / n_trials, .keep = "unused") %>%
  ungroup()

# Descriptives
er_desc <- er %>%
  group_by(probe_type) %>%
  get_summary_stats(er, type = "median_iqr")

# One-sided wilcoxon signed-rank test
er_wcx <- er %>%
  wilcox_test(
    er ~ probe_type,
    paired = TRUE,
    alternative = "greater",
    ref.group = "implied")

# Wilcoxon effect size r
er_wcx_r <- er %>%
  wilcox_effsize(er ~ probe_type,
    paired = TRUE, ref.group = "implied") %>%
  pull(effsize)

# Print wilcoxon test
er_wcx_report <- paste0("Z = ", forma(er_wcx_r * sqrt(nrow(er) / 2)),
  ", ", formp(er_wcx$p, TRUE), ", r = ", forma(er_wcx_r))
er_wcx_plot <- paste0("Z = ", forma(er_wcx_r * sqrt(nrow(er) / 2)),
  ",\n", formp(er_wcx$p, TRUE), ",\nr = ", forma(er_wcx_r))
```

We conducted an exploratory error rate analysis to test whether participants not only responded more slowly in the implied condition but also made more errors.

#### QQ-Plot
```{r er qq, message = FALSE}
er_normality_plot <- er %>%
  pivot_wider(names_from = probe_type, values_from = er) %>%
  mutate(diff = implied - `implied other`) %>%
  ggplot(., aes(sample = diff)) +
  labs(x = "Theoretical quantiles", y = "Data quantiles") +
  stat_qq(color = "#7A8450") +
  stat_qq_line(color = "#7A8450") +
  theme_cs_talk()
er_normality_plot
```

Upon visual inspection the differences between the implied and implied-other conditions seemed severely non-normal. We therefore conducted a wilcoxon signed-rank test.

#### Skewness
```{r er skewness, warning = FALSE, message = FALSE}
er_skewness_plot <- er %>%
  pivot_wider(names_from = probe_type, values_from = er) %>%
  mutate(diff = implied - `implied other`) %>%
  ggplot(., aes(x = diff)) +
  geom_histogram(color = "#7A8450", fill = "white") +
  labs(x = "Differences in error rates", y = "Number of observations") +
  theme_cs_talk()
er_skewness_plot
```

Upon visual inspection the differences between the implied and implied-other condition seemed severely skewed. Although this violates one of its assumptions, we still computed a wilcoxon signed-rank test.

#### Wilcoxon test
A wilcoxon signed-rank test showed that participants' error rates were significantly higher in the implied condition (Mdn = `r forma(er_desc$median[1])`) than in the implied other condition (Mdn = `r forma(er_desc$median[2]) `), `r er_wcx_report`.

#### Descriptives
```{r er descriptives}
knitr::kable(er_desc, format = "markdown")
```

#### Barplot
```{r er barplot, message = FALSE}
er_barplot <- er_desc %>%
  ggplot(., aes(probe_type, median)) +
  labs(x = "Probe type", y = "False recognition rate") +
  geom_bar(stat = "identity", color = "#7A8450", fill = "white") +
  coord_cartesian(ylim = c(0, 0.07)) +
  geom_label(aes(x = 2.19, y = 0.0635, label = er_wcx_plot),
    fill = "white", color = "#7A8450", show.legend = FALSE,
    label.padding = unit(0.5, "lines"), label.size = 0.5) +
  theme_cs_talk()
er_barplot
```

#### Speed-accuracy trade-off
```{r speed accuracy trade-off, message = FALSE}
# Merge response latency and error rate data
d_trade_off <- rt_ex %>%
  left_join(er, by = c("subject_id", "probe_type")) %>%
  select(subject_id, probe_type, rt = rt_M2SD_log, er) %>%
  pivot_wider(names_from = probe_type, values_from = c(er, rt)) %>%
  mutate(
    diff_er = er_implied - `er_implied other`,
    diff_rt = rt_implied - `rt_implied other`, .keep = "unused") %>%
  drop_na()

# Correlation test
trade_off <- cor_test(data = d_trade_off, diff_rt, diff_er)

# Print correlation test
trade_off_report <- paste0("r(", nrow(d_trade_off) - 2, ") = ", forma(trade_off$cor),
  ", ", formp(trade_off$p, TRUE))
```

If there were a trade-off between the speed and accuracy of the participants' responses, one would expect the SII-effects in the reaction times and error rates to be negatively correlated, such that participants with smaller effects in one outcome show greater effects in the other. A Pearson correlation coefficient was computed to assess the linear relationship between the SII-effects in the reaction times and error rates. There was no significant correlation between the two outcome variables, `r trade_off_report`, indicating that participants showing a smaller effect in the reaction times did not show a significantly larger effect in the error rates.

```{r speed accuracy trade-off plot, message = FALSE}
trade_off_plot <- d_trade_off %>%
  ggplot(., aes(x = diff_rt, y = diff_er)) +
  labs(x = "SII effect in response latencies",
      y = "SII effect in error rates (jittered)") +
  geom_jitter(size = 2, shape = 16, height = 0.005, color = "#7A8450") +
  theme_cs_talk()
trade_off_plot
```

### Exploratory by-item and analyses {.tabset}
We conducted exploratory by-item analyses. We grouped the data by label and probe type and aggregated the response latencies (using an individual mean plus two standard deviations cutoff for slow responses and the log-transformation). For each item we calculated the SII-effect as a difference score (implied minus implied-other), with higher values indicating stronger inference effects.

It should be noted, that the difference scores could be calculated in two ways, differing with regards to what is defined as the implied-other label. As we presented two ideological labels after each statement - one implied and one not implied (that is implied in another statement) - the obvious way would be to take the difference between these two labels. However, with the length and currency of the two labels being potentially dissimilar, these differences might be confounded. The less obvious but in our opinion superior way is to take the difference between an implied label and the same label serving as an implied-other label in a different item. This way by-item effects are not confounded by word length or currency.

#### Correlations by label
```{r by-label correlation, message = FALSE}
label_rate <- d_long %>%
  filter(probe_type == "im") %>%
  select(
    label,
    identity = rating_identity,
    valence = rating_valence,
    availability = rating_availability) %>%
  group_by(label) %>%
  summarize(.groups = "drop",
    across(c(identity, valence, availability), \(x) mean(x, na.rm = TRUE)))
label <- d_long %>%
  select(label, behavior, probe_type, rt = rt_M2SD_log, is_correct) %>%
  mutate(across(rt, ~ifelse(is_correct == 0, NA, .)), .keep = "unused") %>%
  group_by(label, behavior, probe_type) %>%
  summarize(.groups = "drop", rt = mean(rt, na.rm = TRUE)) %>%
  pivot_wider(names_from = probe_type, values_from = c(rt, behavior)) %>%
  mutate(sii_effect = rt_im - rt_io, .keep = "unused") %>%
  left_join(label_rate, by = "label")

c1 <- label %>%
  select(sii_effect, identity, valence, availability) %>%
  cor_table()

knitr::kable(c1, format = "markdown")
```

The SII-effect was not significantly correlated with any of the ratings. Descriptively the identity ratings most strongly predicted the SII-effect.

#### Identity
```{r by-label sii identity, message = FALSE, warning = FALSE}
by_item_identity <- label %>%
  ggplot(., aes(x = identity, y = sii_effect)) +
  labs(x = "Mean identity rating", y = "SII effect") +
  geom_abline(intercept = 0, slope = 0, color = "#7A8450") +
  geom_point(color = "#7A8450") +
  geom_label_repel(size = 4.3, label = label$label, box.padding = 0.2,
    family = "Nunito", color = "#7A8450") +
  theme_cs_talk()
by_item_identity
```

#### Valence
```{r by-label sii valence, message = FALSE, warning = FALSE}
by_item_valence <- label %>%
  ggplot(., aes(x = valence, y = sii_effect)) +
  labs(x = "Mean valence rating", y = "SII effect") +
  geom_abline(intercept = 0, slope = 0, color = "#7A8450") +
  geom_point(color = "#7A8450") +
  geom_label_repel(size = 4.3, label = label$label, box.padding = 0.2,
    family = "Nunito", color = "#7A8450") +
  theme_cs_talk()
by_item_valence

```

#### Availability
```{r by-label availability, message = FALSE, warning = FALSE}
by_item_availability <- label %>%
  ggplot(., aes(x = availability, y = sii_effect)) +
  labs(x = "Mean availability rating", y = "SII effect") +
  geom_abline(intercept = 0, slope = 0, color = "#7A8450") +
  geom_point(color = "#7A8450") +
  geom_label_repel(size = 4.3, label = label$label, box.padding = 0.2,
    family = "Nunito", color = "#7A8450") +
  theme_cs_talk()
by_item_availability
```

### Exploratory multilevel analyses {.tabset}
```{r multilevel analyses label, message = FALSE}
d_mlm_label <- d_long %>%
  # Set RTs of incorrect responses to NA
  mutate(rt = ifelse(is_correct == 0, NA, rt_M2SD_none), .keep = "unused") %>%
  select(
    subject_id, item_id, label, probe_type, rt, starts_with("rating_"),
    age, gender, starts_with("pol_")) %>%
  pivot_wider(
    names_from = probe_type,
    values_from = c(matches("rt|rating|item_id")),
    names_sep = "_") %>%
  mutate(
    sii_effect = rt_im - rt_io,
    gender = na_if(gender, "other"),
    # Grand mean center level 2 predictors
    age = age - mean(age, na.rm = TRUE),
    pol_orientation = pol_orientation - mean(pol_orientation),
    pol_interest = pol_interest - mean(pol_interest),
    pol_satisfaction = pol_satisfaction - mean(pol_satisfaction)
  ) %>%
  group_by(subject_id) %>%
  mutate(
    # Group mean center level 1 predictors
    availability = rating_availability_im - mean(rating_availability_im),
    valence = rating_valence_im - mean(rating_valence_im),
    identity = rating_identity_im - mean(rating_identity_im)
  ) %>%
  ungroup() %>%
  left_join(
    select(label, -behavior_im, -behavior_io, -sii_effect),
    by = "label", suffix = c("", "_label")) %>%
  mutate(
    # Grand mean center level 2 predictors
    identity_label = identity_label - mean(identity_label),
    valence_label = valence_label - mean(valence_label),
    availability_label = availability_label - mean(availability_label)) %>%
  select(-matches("rating_|rt_"))

c2 <- d_mlm_label %>%
  select(sii_effect, identity, valence, availability) %>%
  cor_table()
```

We performed exploratory multilevel analyses to investigate whether the ratings regarding the availability and valence of the labels as well as the identification with the labels could explain the strength of the SII-effect. We used difference scores calculated from two raw response latencies (implied minus implied-other) as our dependent variable (again using an individual mean plus two standard deviations cutoff for slow responses and the log-transformation). As in the by-item analyses we calculated the SII-effects by taking the difference between an implied label and the same label serving as an implied-other label to prevent confounding by word length or currency. We first calculated zero-order correlations. The SII-effect was negatively correlated with identity ratings and valence ratings. All ratings were significantly intercorrelated.

#### Zero-order by label
```{r zero-order label}
knitr::kable(c2, format = "markdown")
```

```{r models}
mlm1 <- lmer(sii_effect ~ identity +
        (1 | subject_id) + (1 | label),
        data = d_mlm_label, REML = FALSE)
mlm2 <- lmer(sii_effect ~ valence +
        (1 | subject_id) + (1 | label),
        data = d_mlm_label, REML = FALSE)
mlm3 <- lmer(sii_effect ~ availability +
        (1 | subject_id) + (1 | label),
        data = d_mlm_label, REML = FALSE)
```

#### Model I
```{r multilevel model 1}
sjPlot::tab_model(mlm1)
```

#### Model II
```{r multilevel model 2}
sjPlot::tab_model(mlm2)
```

#### Model III
```{r multilevel model 3}
sjPlot::tab_model(mlm3)
```

```{r save plots, message = FALSE, eval = FALSE}
ggexport(rt_outlier_plot, width = 5, height = 5,
  filename = here("Analysis/rt-outlier-plot.pdf"))
ggexport(rt_qq_plot, width = 5, height = 5,
      filename = here("Analysis/rt-qq-plot.pdf"))
ggexport(rt_barplot, width = 4, height = 5,
      filename = here::here("Analysis/rt-barplot.pdf"))
ggexport(er_normality_plot, width = 5, height = 5,
      filename = here::here("Analysis/er-normality-plot.pdf"))
ggexport(er_skewness_plot, width = 5, height = 5,
      filename = here::here("Analysis/er-skewness-plot.pdf"))
ggexport(er_barplot, width = 4, height = 5,
      filename = here::here("Analysis/er-barplot.pdf"))
ggexport(trade_off_plot, width = 7, height = 6,
      filename = here::here("Analysis/trade-off-plot.pdf"))
ggexport(by_item_identity, width = 7, height = 6,
      filename = here::here("Analysis/by-item-identity.pdf"))
ggexport(by_item_availability, width = 7, height = 6,
      filename = here::here("Analysis/by-item-availability.pdf"))
ggexport(by_item_valence, width = 7, height = 6,
      filename = here::here("Analysis/by-item-valence.pdf"))
```