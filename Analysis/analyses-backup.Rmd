---
title: "Analyses for SII Study 1"
output: 
  html_notebook:
  code_folding: hide
---

```{r setup, include = FALSE}
# R version 4.2.1
# load packages & set options
library(dplyr) # dplyr_1.0.10
library(ggplot2) # ggplot2_3.3.6
library(ggpubr) # ggpubr_0.4.0
library(ggrepel, include.only = "geom_label_repel") # ggrepel_0.9.1
library(here) # here_1.0.1
library(lmerTest) # lmerTest_3.1-3
library(magrittr, include.only = "%T>%") # magrittr_2.0.3
library(rstatix) # rstatix_0.7.0
library(showtext) # showtext_0.9-5
library(tidyverse) # tidyverse_1.3.2

# Read data
d_long <- readRDS(here::here("Processed data/d-long.rds"))

# Demographics
dems <- d_long %>%
  select(subject_id, age, gender, starts_with("pol_")) %>%
  unique()

# Add custom font for plots
font_add("Nunito",
  regular = "/Users/carsten/Library/Fonts/NunitoSans-Regular.ttf",
  italic = "/Users/carsten/Library/Fonts/NunitoSans-Italic.ttf",
  bold = "/Users/carsten/Library/Fonts/NunitoSans-Bold.ttf",
  bolditalic = "/Users/carsten/Library/Fonts/NunitoSans-BoldItalic.ttf")
showtext_auto()

# Custom functions
formp <- function(p, text = FALSE) {
  ## ---------------------------
  ## Format p values
  ##
  ## This function takes in a number between
  ## zero and one or a formatted p-value and outputs
  ## a formatted p-value. If p-value is already formatted
  ## then applying the function changes the format from
  ## "p = .034" to ".034" and vice versa.
  ##
  ## @p p-value to be formatted
  ## @text adds "p = " or "p < " to output
  ##
  ## @out string with formatted p-value
  ## ---------------------------

  # If already formatted but no "p" then add "p"
  if (grepl("^<.\\d{3}$", p)) {
  out <- gsub("<", "p < ", p)
  } else if (grepl("^.\\d{3}$", p)) {
  out <- gsub("^", "p = ", p)
  # If already formatted and "p" then remove "p"
  } else if (grepl("^p < .\\d{3}$", p)) {
  out <- gsub("p < ", "<", p)
  } else if (grepl("^p = .\\d{3}$", p)) {
  out <- gsub("p = ", "", p)
  # If not yet formatted and smaller than .001
  } else if (is.numeric(p) && p < 0.001) {
  if (text) {
    out <- "p < .001"
  } else {
    out <- "<.001"
  }
  # If not yet formatted and bigger than .001
  } else if (p >= 0.001) {
  p <- format(round(p, 3), nsmall = 3, scientific = FALSE)
  p <- sub("0.", ".", p)
  if (text) {
    out <- paste("p =", p)
  } else {
    out <- p
  }
  }
  return(out)
}
forma <- function(number, dec = NULL, lead_zero = TRUE) { # nolint
  ## ---------------------------
  ## Format values in apa style
  ##
  ## This function takes in a number and outputs
  ## a formatted number. If no decimal is provided, then
  ## it uses a heuristic to round the number. If lead_zero
  ## is set to FALSE, then the lead zero of the number is
  ## removed (useful for p-values or eta squared).
  ##
  ## @number input number
  ## @dec optional number of decimals
  ## @lead_zero keep leading zero
  ##
  ## @out formatted number
  ## ---------------------------

  # If dec is logical, interpret as lead_zero
  if (is.logical(dec)) {
  lead_zero <- dec
  dec <- NULL
  }
  # If no decimal is specified, use heuristic
  if (!is.null(dec)) {
  } else if (abs(number) >= 100) {
    dec <- 0
  } else if (abs(number) >= 10 && number < 100) {
    dec <- 1
  } else if (abs(number) >= 0.1 && number < 10) {
    dec <- 2
  } else if (abs(number) >= 0.001 && number < 0.1) {
    dec <- 3
  } else if (abs(number) < 0.001 && number != 0) {
    dec <- stringr::str_locate(format(
      abs(number), scientific = FALSE), "[1-9]{1}")[1] - 2
  } else if (number == 0) {
    dec <- 0
  }
  # Round number to decimal
  out <- format(round(number, dec), nsmall = dec, scientific = FALSE)
  # Remove leading zero if required
  if (out < 1 && lead_zero == FALSE) {
  out <- sub("0.", ".", out)
  }
  return(out)
}
cor_table <- function(data, method = c("pearson", "spearman")) {
  # Compute correlation matrix
  pvalues <- data %>%
  cor_pmat(method = method[1]) %>%
  rowwise() %>%
  mutate(across(!1, formp))
  coefs <- data %>%
  cor_mat(method = method[1]) %>%
  rowwise() %>%
  mutate(across(!1, forma, 2))
  for (row in seq(2, nrow(coefs))) {
  for (col in seq(2, ncol(coefs) - 1)) {
    c <- coefs[row, col]
    p <- pvalues[row, col]
    coefs[row, col] <- paste0(c, " (", p, ")")
  }
  }
  coefs <- coefs %>%
  pull_lower_triangle() %>%
  slice(-1) %>%
  select(-last_col()) %>%
  rename(variable = 1)
  return(coefs)
}
mlm_compare <- function(new, old) {
  ## ---------------------------
  ## Compare two multilevel models
  ##
  ## Outputs a list with a test comparing the models ($test),
  ## a formatted p-value ($p), the variance explained by the
  ## random and fixed effects ($var_exp) and the percentage
  ## increase in variance explained in the new compared to
  ## the old model.
  ##
  ## @new new model
  ## @old old model to compare the new
  ##    model against
  ##
  ## @out list with statistics
  ## ---------------------------

  out <- list()

  # Model comparison
  out$test <- anova(new, old)
  out$p <- formp(out$test[2, "Pr(>Chisq)"], TRUE)

  # Stats new model
  var_new <- as.data.frame(lme4::VarCorr(new))
  var_exp <- data.frame(row.names = c("old", "new"))
  # Loop across random effects
  for (i in seq_along(var_new[, 1])) {
  var_exp_i <- var_new$vcov[i] / sum(var_new$vcov) * 100
  var_exp[2, var_new$grp[i]] <- var_exp_i
  }

  # Stats old model
  if (class(old) != "lm") {
  var_old <- as.data.frame(lme4::VarCorr(old))
  # Loop across random effects
  for (i in seq_along(var_old[, 1])) {
    var_exp_i <- var_old$vcov[i] / sum(var_old$vcov) * 100
    var_exp[1, var_old$grp[i]] <- var_exp_i
  }
  var_exp["diff", ] <- var_exp[1, ] - var_exp[2, ]
  var_exp_delta <- ((var_exp[3, ] / var_exp[1, ]) * 100)
  var_exp_delta <- sapply(var_exp_delta, forma)
  var_exp[] <- apply(var_exp, c(1, 2), forma)
  out$var_exp_delta <- var_exp_delta
  } else if (class(old) == "lm") {
  var_exp[2, ] <- sapply(var_exp[2, ], forma)
  }
  out$var_exp <- var_exp
  return(out)
}
theme_cs_talk <- function(font = "Nunito", lab_size = 16, label_size = 14,
  dark = "#7A8450", light = "#F7FF8C", solid_facet = TRUE) {
  if (solid_facet) {
  facet_fill <- dark
  facet_text <- light
  } else if (!solid_facet) {
  facet_fill <- "transparent"
  facet_text <- dark
  }
  theme_bw(base_size = 16) %+replace%
  theme(
  # Rectangle elements
  plot.background = element_rect(fill = "transparent",
    color = NA_character_),
  panel.background = element_rect(fill = "transparent"),
  legend.background = element_rect(fill = "transparent", color = NA),
  strip.background = element_rect(color = facet_fill,
    fill = facet_fill, size = 1),
  # Text elements
  plot.title = element_text(family = font, size = lab_size,
    face = "bold", hjust = 0, vjust = 2, color = dark),
  plot.subtitle = element_text(family = font,
    size = lab_size - 2, color = dark),
  plot.caption = element_text(family = font, size = lab_size,
    hjust = 1, color = dark),
  axis.title = element_text(family = font, size = lab_size,
    color = dark),
  axis.text = element_text(family = font, size = label_size,
    color = dark),
  axis.text.x = element_text(margin = margin(5, b = 10),
    color = dark),
  legend.title = element_text(family = font, size = lab_size,
    color = dark, hjust = 0),
  legend.text = element_text(family = font, size = label_size,
    color = dark),
  strip.text = element_text(family = font, size = label_size,
    color = facet_text, margin = margin(4, 4, 4, 4)),
  # Line elements
  axis.ticks = element_line(color = dark, size = 0.5),
  legend.key = element_rect(fill = "transparent", color = NA_character_),
  panel.grid.major = element_blank(),
  panel.grid.minor = element_blank(),
  panel.border = element_rect(color = dark, fill = NA, size = 1)
  )
}
```

### Introduction
Research on person perception has revealed that people tend to attribute other's behavior to stable person characteristics, even if it can just as well be explained by reasons such as situational factors or mental states. This tendency is known as the correspondence bias (Gilbert & Malone, 1995) and may play an important role in the domain of political polarization. Specifically, people may attribute each other's politically relevant behaviors to stable ideological dispositions (such as leftist, conservative, racist, or feminist), while neglecting potential other causes and thereby impeding mutual understanding. To investigate the role of the correspondence bias in political polarization we first examine whether ideological dispositions are spontaneously inferred from behavioral descriptions, thus extending previous research on spontaneous trait inferences (STI, Winter & Uleman, 1984) to spontaneous inferences of ideological dispositions. We expect response latencies of correct rejections of implied probes (i.e., ideological probe words implied by but not included in the sentence) to be slower than of probes that were not implied in the sentence (but rather in another sentence of the task).

### Sample Characteristics
We collected data from a total of N = 170 participants. Following our pre-registered exclusion criteria, we excluded 5 participants whose average correct response times were slower than two standard deviations over the sample mean, 1 who self-reported not having followed the instructions conscientiously, and 1 who rated their own data to be unfit for analysis. This resulted in a sample of N = `r nrow(dems)` participants (`r nrow(filter(dems, gender == "female"))` female, `r nrow(filter(dems, gender == "male"))` male, `r nrow(filter(dems, gender == "other"))` other, `r nrow(filter(dems, gender %in% c("not specified","")))` not specified; average age M = `r round(mean(dems$age, na.rm = TRUE), 1)` years, SD = `r round(sd(dems$age, na.rm = TRUE), 1)`, ranging from `r min(dems$age, na.rm = TRUE)` to `r max(dems$age, na.rm = TRUE)`). Participants were recruited via the online platform Prolific (www.prolific.co) and received monetary compensation of 3.13 GBP for completing the 25-minute study. An additional 27 people started the experiment on prolific but either returned their submission, timed-out, or only partially completed the experiment due to technical issues.

On average, participants reported to be rather left leaning (M = `r round(mean(dems$pol_orientation), 1)`, SD = `r round(sd(dems$pol_orientation), 1)` on a scale from 1 = left to 10 = right), rather interested in politics (M = `r round(mean(dems$pol_interest), 1)`, SD = `r round(sd(dems$pol_interest), 1)`, on a scale ranging from 1 = not at all to 10 = very strongly), and moderately satisfied with the German political system (M = `r round(mean(dems$pol_satisfaction), 1)`, SD = `r round(sd(dems$pol_satisfaction), 1)`, on a scale ranging from 1 = satisfied to 4 = dissatisfied).

### Preregistered analysis {.tabset}
```{r rt analysis, warning = FALSE, message = FALSE}
# Prepare reaction time data
rt <- d_long %>%
  mutate(across(starts_with("rt_"), ~ifelse(is_correct == 0, NA, .))) %>%
  select(subject_id, probe_type, starts_with("rt_")) %>%
  mutate(probe_type = recode(probe_type,
    "im" = "implied", "io" = "implied other")) %>%
  group_by(probe_type, subject_id) %>%
  summarize(.groups = "drop_last", across(everything(), mean, na.rm = TRUE)) %>%
  # Find extreme outliers
  mutate(is_extreme = is_extreme(rt_M2SD_log)) %>%
  ungroup() %T>%
  # Count participants that have extreme outliers
  assign(x = "rt_ex_n", envir = .GlobalEnv, value = n_distinct(
    select(filter(., is_extreme == TRUE), subject_id)))

# Create data frames where these rows are excluded
rt_ex <- filter(rt, is_extreme == FALSE)

# Descriptives
rt_desc <- rt_ex %>%
  group_by(probe_type) %>%
  mutate(rt_M2SD_log_exp = exp(rt_M2SD_log)) %>%
  get_summary_stats(rt_M2SD_log_exp, type = "mean_sd") %>%
  mutate(
    ci95_low = mean - 1.96 * sd / sqrt(n),
    ci95_upp = mean + 1.96 * sd / sqrt(n))

# One-sided paired t-test
rt_t <- rt_ex %>%
  t_test(
    rt_M2SD_log ~ probe_type,
    paired = TRUE,
    alternative = "greater",
    ref.group = "implied")

# Cohens dz
rt_dz <- rt_ex %>%
  cohens_d(rt_M2SD_log ~ probe_type,
    paired = TRUE, ref.group = "implied") %>%
  pull(effsize)

# Print t-test
rt_t_report <- paste0("t(", rt_t$n1, ") = ", forma(rt_t$statistic),
  ", ", formp(rt_t$p, TRUE), ", d~z~ = ", forma(rt_dz))
rt_t_plot <- paste0("t(", rt_t$n1, ") = ", forma(rt_t$statistic),
  ",\n", formp(rt_t$p, TRUE), ",\nd~z~ = ", forma(rt_dz))
```

To prepare the response latency data for our main anlysis we applied an individual cut-off of the individual mean plus two standard deviations for slow responses and the log-transformation and then computed the average for each experimental condition. For reports of descriptive data we backtransformed the logarithmized values by calculating their exponentials.

#### Outlier detection
```{r rt outlier, message = FALSE}
rt %>%
  {
    ggplot(., aes(probe_type, rt_M2SD_log)) +
    geom_boxplot(color = "#7A8450") +
    labs(x = "Probe type", y = "Mean response latency in log(s)") +
    theme_cs_talk()
  } %T>%
  ggexport(., width = 5, height = 5,
       filename = here("Analysis/rt-outlier-plot.pdf"))
```

We looked for extreme outliers in each cell of the design. We defined them as values above Q3 + 3 * IQR or below Q1 - 3 * IQR. We found and excluded `r rt_ex_n` participants whose values were extreme outliers.

#### QQ-Plot
```{r rt qq, message = FALSE}
rt_ex %>%
  pivot_wider(names_from = probe_type, values_from = starts_with("rt_")) %>%
  mutate(diff = rt_M2SD_log_implied - `rt_M2SD_log_implied other`) %>%
  {
    ggplot(., aes(sample = diff)) +
    labs(x = "Theoretical quantiles", y = "Data quantiles") +
    stat_qq(color = "#7A8450") +
    stat_qq_line(color = "#7A8450") +
    theme_cs_talk()
  } %T>%
  ggexport(width = 5, height = 5,
       filename = here("Analysis/rt-qq-plot.pdf"))
```

Upon visual inspection the differences between the implied and implied-other conditions did not seem severely non-normal. We therefore applied no further transformations before conducting a paired t-test.

#### T-test
On average, participants responded significantly slower in the implied condition (M = `r forma(rt_desc$mean[1])`, SD = `r forma(rt_desc$sd[1])`) than in the implied other condition (M = `r forma(rt_desc$mean[2])`, SD = `r forma(rt_desc$sd[2])`), `r rt_t_report`. This indicates that participants spontaneously activated the implied labels while reading the statements.

#### Descriptives
```{r rt descriptives}
knitr::kable(rt_desc, format = "markdown")
```

#### Barplot
```{r rt barplot, message = FALSE}
rt_desc %>%
  {
    ggplot(., aes(probe_type, mean)) +
    labs(x = "Probe type", y = "Mean response latency in seconds") +
    geom_bar(stat = "identity", color = "#7A8450", fill = "white") +
    coord_cartesian(ylim = c(0.65, 0.87)) +
    geom_linerange(aes(ymin = ci95_low, ymax = ci95_upp), color = "#7A8450") +
    geom_label(aes(x = 2.08, y = 0.85, label = rt_t_plot),
      fill = "white", color = "#7A8450", show.legend = FALSE,
      label.padding = unit(0.5, "lines"), label.size = 0.5) +
    theme_cs_talk()
  } %T>%
  ggexport(width = 4, height = 5,
       filename = here::here("Analysis/rt-barplot.pdf"))
```

#### Multiverse
```{r rt mv, message = FALSE}
rt_mv <- d_long %>%
  mutate(across(starts_with("rt_"), ~ifelse(is_correct == 0, NA, .))) %>%
  select(probe_type, subject_id, starts_with("rt_")) %>%
  mutate(probe_type =
    recode(probe_type, "im" = "implied", "io" = "implied other")) %>%
  group_by(probe_type, subject_id) %>%
  summarize(.groups = "drop", across(everything(), mean, na.rm = TRUE)) %>%
  pivot_longer(
    cols = starts_with("rt"),
    names_to = c(NA, "cutoff", "trans"),
    values_to = "rt",
    names_sep = "_")

# Loop over all combinations of cutoffs and transformations
mv <- data.frame(
  cutoff = rep(c("none", "M2SD", "f25", "f20", "f15"), 3),
  transformation = c(rep("none", 5), rep("log", 5), rep("inv", 5)))
for (c in seq_along(mv[, 1])) {
  dat <- rt_mv %>%
    # Filter data using c-th cutoff and transformation
    filter(
      cutoff == mv$cutoff[c] &
      trans == mv$transformation[c]) %>%
    group_by(probe_type) %>%
    mutate(is_extreme = is_extreme(rt)) %>%
    ungroup() %T>%
    assign(x = "ex_n", envir = .GlobalEnv, value = n_distinct(
      select(filter(., is_extreme == TRUE), subject_id))) %>%
    filter(is_extreme == FALSE) %>%
    group_by(subject_id) %>%
    filter(n() == 2) %>%
    ungroup()
  test <- dat %>%
    t_test(rt ~ probe_type, paired = TRUE,
      alternative = "greater", ref.group = "implied") %>%
    rowwise() %>%
    mutate(statistic = forma(statistic, 2), p = formp(p))
  dz <- dat %>%
    cohens_d(rt ~ probe_type, paired = TRUE, ref.group = "implied") %>%
    pull(effsize) %>%
    forma()
  # Save test statistics
  mv$ex_n[c] <- ex_n
  mv$t[c] <- test$statistic
  mv$df[c] <- test$df
  mv$p[c] <- test$p
  mv$d_z[c] <- dz
}
# Print multiverse table
knitr::kable(mv, format = "markdown")
```

Because there are no conventions regarding outlier correction and transformations, we follow a recommendation by Krieglmeyer & Deutsch (2010) and employ a multiverse analysis using different cut-off criteria (no cut-off, 2500 ms, 2000 ms, and 1500 ms) and transformations (no transformation, a log-transformation, and an inverse transformation) and report each combination's effects on the results.

The effect of probe type was highly significant across all combinations of cutoff criteria and transformations. The effect size ranged from d~z~ = `r mv$d_z[1]` for untransformed response latencies with no cutoff to d~z~ = `r mv$d_z[10]` for log transformed response latencies with a 1500 ms cutoff.

### Exploratory error rate analysis {.tabset}
```{r er analysis, message = FALSE}
# Prepare error rate data
er <- d_long %>%
  select(subject_id, probe_type, is_correct) %>%
    mutate(probe_type = recode(probe_type,
      "im" = "implied", "io" = "implied other")) %>%
  group_by(probe_type, subject_id) %>%
  summarize(
    .groups = "drop_last",
    errors = sum(is_correct == 0),
    n_trials = n()) %>%
  mutate(er = errors / n_trials, .keep = "unused") %>%
  # Find extreme outliers
  mutate(is_extreme = is_extreme(er)) %>%
  ungroup() %T>%
  # Count participants that have extreme outliers
  assign(x = "er_ex_n", envir = .GlobalEnv, value = n_distinct(
    select(filter(., is_extreme == TRUE), subject_id)))

# Create data frame where participants with extreme values are excluded
er_ex <- er %>%
  filter(is_extreme == FALSE) %>%
  group_by(subject_id) %>%
  filter(n() == 2) %>%
  ungroup()

# Descriptives
er_desc <- er_ex %>%
  group_by(probe_type) %>%
  get_summary_stats(er, type = "median_iqr")

# One-sided wilcoxon signed-rank test
er_wcx <- er_ex %>%
  wilcox_test(
    er ~ probe_type,
    paired = TRUE,
    alternative = "greater",
    ref.group = "implied")

# Wilcoxon effect size r
er_wcx_r <- er_ex %>%
  wilcox_effsize(er ~ probe_type,
    paired = TRUE, ref.group = "implied") %>%
  pull(effsize)

# Print wilcoxon test
er_wcx_report <- paste0("Z = ", forma(er_wcx_r * sqrt(nrow(er_ex) / 2)),
  ", ", formp(rt_t$p, TRUE), ", r = ", forma(er_wcx_r))
er_wcx_plot <- paste0("Z = ", forma(er_wcx_r * sqrt(nrow(er_ex) / 2)),
  ",\n", formp(rt_t$p, TRUE), ",\nr = ", forma(er_wcx_r))
```

We conducted an exploratory error rate analysis to test whether participants not only responded more slowly in the implied condition but also made more errors.

#### Outlier detection
```{r er outlier, warning = FALSE, message = FALSE}
er_ex %>%
  {
    ggplot(., aes(probe_type, er)) +
    geom_boxplot(color = "#7A8450") +
    labs(x = "Probe type", y = "False recognition rate") +
    geom_jitter(
      # Replace all non-outlier values by NA
      data = mutate(er, er = replace(er, is_extreme == FALSE, NA)),
      width = 0.25, height = 0.007, shape = 4,
      size = 4, show.legend = FALSE) +
    theme_cs_talk()
  } %T>%
  ggexport(width = 5, height = 5,
       filename = here::here("Analysis/er-outlier-plot.pdf"))
```

We looked for extreme outliers in each cell of the design. We defined them as values above Q3 + 3 * IQR or below Q1 - 3 * IQR. We found and excluded `r er_ex_n` participants whose values were extreme outliers. 

#### QQ-Plot
```{r er qq, message = FALSE}
er_ex %>%
  pivot_wider(names_from = probe_type, values_from = er) %>%
  mutate(diff = implied - `implied other`) %>%
  {
    ggplot(., aes(sample = diff)) +
    labs(x = "Theoretical quantiles", y = "Data quantiles") +
    stat_qq(color = "#7A8450") +
    stat_qq_line(color = "#7A8450") +
    theme_cs_talk()
  } %T>%
  ggexport(width = 5, height = 5,
       filename = here::here("Analysis/er-normality-plot.pdf"))
```

Upon visual inspection the differences between the implied and implied-other conditions seemed severely non-normal. We therefore conducted a wilcoxon signed-rank test.

#### Wilcoxon test
A wilcoxon signed-rank test showed that participants' error rates were significantly higher in the implied condition (Mdn = `r forma(er_desc$median[1])`) than in the implied other condition (Mdn = `r forma(er_desc$median[2]) `), `r er_wcx_report`.

#### Descriptives
```{r er descriptives}
knitr::kable(er_desc, format = "markdown")
```

#### Barplot
```{r er barplot, message = FALSE}
er_desc %>%
  {
    ggplot(., aes(probe_type, median)) +
    labs(x = "Probe type", y = "False recognition rate") +
    geom_bar(stat = "identity", color = "#7A8450", fill = "white") +
    coord_cartesian(ylim = c(0, 0.07)) +
    geom_label(aes(x = 2.19, y = 0.0635, label = er_wcx_plot),
      fill = "white", color = "#7A8450", show.legend = FALSE,
      label.padding = unit(0.5, "lines"), label.size = 0.5) +
    theme_cs_talk()
  } %T>%
  ggexport(width = 4, height = 5,
       filename = here::here("Analysis/er-barplot.pdf"))
```

#### Speed-accuracy trade-off
```{r speed accuracy trade-off, message = FALSE}
# Merge response latency and error rate data
d_trade_off <- rt_ex %>%
  left_join(er_ex, by = c("subject_id", "probe_type")) %>%
  select(subject_id, probe_type, rt = rt_M2SD_log, er) %>%
  pivot_wider(names_from = probe_type, values_from = c(er, rt)) %>%
  mutate(
    diff_er = er_implied - `er_implied other`,
    diff_rt = rt_implied - `rt_implied other`, .keep = "unused") %>%
  drop_na()

# Correlation test
trade_off <- cor_test(data = d_trade_off, diff_rt, diff_er)

# Print correlation test
trade_off_report <- paste0("r(", nrow(d_trade_off) - 2, ") = ", forma(trade_off$cor),
  ", ", formp(trade_off$p, TRUE))
```

If there were a trade-off between the speed and accuracy of the participants' responses, one would expect the SII-effects in the reaction times and error rates to be negatively correlated, such that participants with smaller effects in one outcome show greater effects in the other. A Pearson correlation coefficient was computed to assess the linear relationship between the SII-effects in the reaction times and error rates. There was no significant correlation between the two outcome variables, `r trade_off_report`, indicating that participants showing a smaller effect in the reaction times did not show a significantly larger effect in the error rates.

```{r speed accuracy trade-off plot, message = FALSE}
d_trade_off %>% 
  {
    ggplot(., aes(x = diff_rt, y = diff_er)) +
    labs(x = "SII effect in response latencies",
        y = "SII effect in error rates (jittered)") +
    geom_jitter(size = 2, shape = 16, height = 0.005, color = "#7A8450") +
    theme_cs_talk()
  } %T>%
  ggexport(width = 7, height = 6,
       filename = here::here("Analysis/trade-off-plot.pdf"))
```

### Exploratory by-item and by-label analyses {.tabset}
We conducted exploratory by-item analyses. The data could be grouped either by item (controlling for properties of the behavioral information) or by label (controlling for properties of the label probe). Because either analysis can be informative, we present both.

#### Correlations by item
```{r by-item correlation, message = FALSE}
item_rate <- d_long %>%
  filter(probe_type == "im") %>%
  select(
    item_id,
    identity = rating_identity,
    valence = rating_valence,
    availability = rating_availability) %>%
  group_by(item_id) %>%
  summarize(.groups = "drop",
    across(c(identity, valence, availability), mean, na.rm = TRUE))
item <- d_long %>%
  select(item_id, behavior, probe_type, rt = rt_M2SD_log, is_correct) %>%
  mutate(across(rt, ~ifelse(is_correct == 0, NA, .)), .keep = "unused") %>%
  group_by(item_id, behavior, probe_type) %>%
  summarize(.groups = "drop", rt = mean(rt, na.rm = TRUE)) %>%
  pivot_wider(names_from = probe_type, values_from = rt) %>%
  mutate(sii_effect = im - io, .keep = "unused") %>%
  left_join(item_rate)

c1 <- item %>%
  select(sii_effect, identity, valence, availability) %>%
  cor_table()

knitr::kable(c1, format = "markdown")
```

For the by-item analysis we grouped the data by item and probe type and aggregated the response latencies (using an individual mean plus two standard deviations cutoff for slow responses and the log-transformation). For each item we calculated the SII-effect as a difference score (implied minus implied-other), with higher values indicating stronger inference effects.

There were some items with negative effects. Specifically, the "afdler_1" item had a negative effect, which might be due to the implied other label being a potentially strong antonym ("links"), that was activated in response to the statement (even more so than the implied label). Furthermore, the "hippie_1" item had a negative effect, potentially also because the implied other label ("konservativ") being implied by (parts of) the statement. The "kommunist_1" item had a null effect, which we could more confidently attribute to the implied other label ("Verschwörungstheoretikerin") being accidentally implied by the statement.

We calculated correlations between the SII-effect and the three ratings (identity, valence, and availability). None of the correlations reached significance.

#### Identity
```{r by-item sii identity, message = FALSE}
item %>%
  {
    ggplot(., aes(x = identity, y = sii_effect)) +
    labs(x = "Mean identity rating", y = "SII effect") +
    geom_abline(intercept = 0, slope = 0, color = "#7A8450") +
    geom_point(color = "#7A8450") +
    geom_label_repel(size = 4.3, label = .$item_id, box.padding = 0.2,
      family = "Nunito", color = "#7A8450") +
    theme_cs_talk()
  } %T>%
  ggexport(width = 7, height = 6,
       filename = here::here("Analysis/by-item-identity.pdf"))
```

#### Valence
```{r by-item sii valence, message = FALSE}
item %>%
  {
    ggplot(., aes(x = valence, y = sii_effect)) +
    labs(x = "Mean valence rating", y = "SII effect") +
    geom_abline(intercept = 0, slope = 0, color = "#7A8450") +
    geom_point(color = "#7A8450") +
    geom_label_repel(size = 4.3, label = .$item_id, box.padding = 0.2,
      family = "Nunito", color = "#7A8450") +
    theme_cs_talk()
  } %T>%
  ggexport(width = 7, height = 6,
       filename = here::here("Analysis/by-item-valence.pdf"))
```

#### Availability
```{r by-item availability, message = FALSE}
item %>%
  {
    ggplot(., aes(x = availability, y = sii_effect)) +
    labs(x = "Mean availability rating", y = "SII effect") +
    geom_abline(intercept = 0, slope = 0, color = "#7A8450") +
    geom_point(color = "#7A8450") +
    geom_label_repel(size = 4.3, label = .$item_id, box.padding = 0.2,
      family = "Nunito", color = "#7A8450") +
    theme_cs_talk()
  } %T>%
  ggexport(width = 7, height = 6,
       filename = here::here("Analysis/by-item-availability.pdf"))
```

#### Correlations by label
```{r by-label correlation, message = FALSE}
label_rate <- d_long %>%
  filter(probe_type == "im") %>%
  select(
    label,
    identity = rating_identity,
    valence = rating_valence,
    availability = rating_availability) %>%
  group_by(label) %>%
  summarize(.groups = "drop",
    across(c(identity, valence, availability), mean, na.rm = TRUE))
label <- d_long %>%
  select(label, behavior, probe_type, rt = rt_M2SD_log, is_correct) %>%
  mutate(across(rt, ~ifelse(is_correct == 0, NA, .)), .keep = "unused") %>%
  group_by(label, behavior, probe_type) %>%
  summarize(.groups = "drop", rt = mean(rt, na.rm = TRUE)) %>%
  pivot_wider(names_from = probe_type, values_from = c(rt, behavior)) %>%
  mutate(sii_effect = rt_im - rt_io, .keep = "unused") %>%
  left_join(label_rate, by = "label")

c2 <- label %>%
  select(sii_effect, identity, valence, availability) %>%
  cor_table()

knitr::kable(c2, format = "markdown")
```

For the by-label analysis we grouped the data by label and probe type and aggregated the response latencies (using an individual mean plus two standard deviations cutoff for slow responses and the log-transformation). For each item we calculated the SII-effect as a difference score (implied minus implied-other), with higher values indicating stronger inference effects.

There was one label with a negative effect, namely the label "links". This might be because when used as an implied other label it was a strong antonym of the implied label. This might have led to the label being activated.

We calculated correlations between the SII-effect and the three ratings (identity, valence, and availability). Again, none of the correlations reached significance.

#### Identity
```{r by-label sii identity, message = FALSE}
label %>%
  {
    ggplot(., aes(x = identity, y = sii_effect)) +
    labs(x = "Mean identity rating", y = "SII effect") +
    geom_abline(intercept = 0, slope = 0, color = "#7A8450") +
    geom_point(color = "#7A8450") +
    geom_label_repel(size = 4.3, label = .$label, box.padding = 0.2,
      family = "Nunito", color = "#7A8450") +
    theme_cs_talk()
  } %T>%
  ggexport(width = 7, height = 6,
       filename = here::here("Analysis/by-item-identity.pdf"))
```

#### Valence
```{r by-label sii valence, message = FALSE}
label %>%
  {
    ggplot(., aes(x = valence, y = sii_effect)) +
    labs(x = "Mean valence rating", y = "SII effect") +
    geom_abline(intercept = 0, slope = 0, color = "#7A8450") +
    geom_point(color = "#7A8450") +
    geom_label_repel(size = 4.3, label = .$label, box.padding = 0.2,
      family = "Nunito", color = "#7A8450") +
    theme_cs_talk()
  } %T>%
  ggexport(width = 7, height = 6,
       filename = here::here("Analysis/by-item-valence.pdf"))
```

#### Availability
```{r by-label availability, message = FALSE}
label %>%
  {
    ggplot(., aes(x = availability, y = sii_effect)) +
    labs(x = "Mean availability rating", y = "SII effect") +
    geom_abline(intercept = 0, slope = 0, color = "#7A8450") +
    geom_point(color = "#7A8450") +
    geom_label_repel(size = 4.3, label = .$label, box.padding = 0.2,
      family = "Nunito", color = "#7A8450") +
    theme_cs_talk()
  } %T>%
  ggexport(width = 7, height = 6,
       filename = here::here("Analysis/by-item-availability.pdf"))
```

### Exploratory multilevel analyses {.tabset}
```{r multilevel analyses item, message = FALSE}
d_mlm_item <- d_long %>%
  # Set RTs of incorrect responses to NA
  mutate(rt = ifelse(is_correct == 0, NA, rt_M2SD_none), .keep = "unused") %>%
  select(
    subject_id, item_id, label, probe_type, rt, starts_with("rating_"),
    age, gender, starts_with("pol_")) %>%
  pivot_wider(
    names_from = probe_type,
    values_from = c(matches("rt|rating|label")),
    names_sep = "_") %>%
  mutate(
    sii_effect = rt_im - rt_io,
    gender = na_if(gender, "other"),
    # Grand mean center level 2 predictors
    age = age - mean(age, na.rm = TRUE),
    pol_orientation = pol_orientation - mean(pol_orientation),
    pol_interest = pol_interest - mean(pol_interest),
    pol_satisfaction = pol_satisfaction - mean(pol_satisfaction)
  ) %>%
  group_by(subject_id) %>%
  mutate(
    # Group mean center level 1 predictors
    availability = rating_availability_im - mean(rating_availability_im),
    valence = rating_valence_im - mean(rating_valence_im),
    identity = rating_identity_im - mean(rating_identity_im)
  ) %>%
  ungroup() %>%
  left_join(
    select(item, -behavior, -sii_effect),
    by = "item_id", suffix = c("", "_item")) %>%
  mutate(
    # Grand mean center level 2 predictors
    identity_item = identity_item - mean(identity_item),
    valence_item = valence_item - mean(valence_item),
    availability_item = availability_item - mean(availability_item)) %>%
  select(-matches("rating_|rt_"))

c3 <- d_mlm_item %>%
  select(sii_effect, identity, valence, availability) %>%
  cor_table()
```

```{r multilevel analyses label, message = FALSE}
d_mlm_label <- d_long %>%
  # Set RTs of incorrect responses to NA
  mutate(rt = ifelse(is_correct == 0, NA, rt_M2SD_none), .keep = "unused") %>%
  select(
    subject_id, item_id, label, probe_type, rt, starts_with("rating_"),
    age, gender, starts_with("pol_")) %>%
  pivot_wider(
    names_from = probe_type,
    values_from = c(matches("rt|rating|item_id")),
    names_sep = "_") %>%
  mutate(
    sii_effect = rt_im - rt_io,
    gender = na_if(gender, "other"),
    # Grand mean center level 2 predictors
    age = age - mean(age, na.rm = TRUE),
    pol_orientation = pol_orientation - mean(pol_orientation),
    pol_interest = pol_interest - mean(pol_interest),
    pol_satisfaction = pol_satisfaction - mean(pol_satisfaction)
  ) %>%
  group_by(subject_id) %>%
  mutate(
    # Group mean center level 1 predictors
    availability = rating_availability_im - mean(rating_availability_im),
    valence = rating_valence_im - mean(rating_valence_im),
    identity = rating_identity_im - mean(rating_identity_im)
  ) %>%
  ungroup() %>%
  left_join(
    select(label, -behavior_im, -behavior_io, -sii_effect),
    by = "label", suffix = c("", "_label")) %>%
  mutate(
    # Grand mean center level 2 predictors
    identity_label = identity_label - mean(identity_label),
    valence_label = valence_label - mean(valence_label),
    availability_label = availability_label - mean(availability_label)) %>%
  select(-matches("rating_|rt_"))

c4 <- d_mlm_label %>%
  select(sii_effect, identity, valence, availability) %>%
  cor_table()
```

We performed exploratory moderation analyses to investigate whether the ratings regarding the availability and valence of the labels as well as the identification with the labels could explain the strength of the SII-effect. We used a difference score (implied minus implied other) of the reaction times (using an individual mean plus two standard deviations cut-off for slow responses and no transformation) as the dependent variable. To test whether a multilevel modelling approach was necessary we built an intercept only model and tested whether a random effect for the participants significantly increased the model fit.

#### Zero-order by item
```{r zero-order item}
knitr::kable(c3, format = "markdown")
```

#### Zero-order by label
```{r zero-order label}
knitr::kable(c4, format = "markdown")
```

```{r models}
# Intercept only
mlm1 <- lm(sii_effect ~ 1, data = d_mlm_label)
# Add random effects
mlm2 <- lmer(sii_effect ~ 1 + (1 | subject_id) + (1 | label),
        data = d_mlm_label, REML = FALSE)
# Add level 1 predictors (item ratings)
mlm3 <- lmer(sii_effect ~ identity + valence + availability +
        (1 | subject_id) + (1 | label),
        data = d_mlm_label, REML = FALSE)
# Add level 2 predictors (mean item ratings)
mlm4 <- lmer(sii_effect ~ identity + valence + availability +
        identity_label + valence_label + availability_label +
        (1 | subject_id) + (1 | label),
        data = d_mlm_label, REML = FALSE)

# Compare models
mlm2_test <- mlm_compare(mlm2, mlm1)
mlm3_test <- mlm_compare(mlm3, mlm2)
mlm4_test <- mlm_compare(mlm4, mlm3)

# Calculate effect size for identity rating
coef <- abs(round(summary(mlm4)$coefficients[, 1] * 1000, 2))
coef_baserate <- coef[names(coef) == "baserate"]
coef_sufficiency <- coef[names(coef) == "sufficiency"]
coef_necessity_item <- coef[names(coef) == "necessity_item"]
coef_sufficiency_item <- coef[names(coef) == "sufficiency_item"]
```

Text

#### Model I
```{r multilevel model 1}
sjPlot::tab_model(mlm2)
```

#### Model II
```{r multilevel model 2}
sjPlot::tab_model(mlm3)
```

#### Model III
```{r multilevel model 3}
sjPlot::tab_model(mlm4)
```